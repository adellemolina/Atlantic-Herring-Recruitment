---
title: "Atl Herring Recruitment BRT"
author: "AM"
date: "2024-02-19"
output: html_document
---

```{r setup, include=T}
knitr::opts_chunk$set(echo = F)
```

# Boosted Regression Trees for herring recruitment

## First Runs with ASAP in 2022

-   No herring specific spatial "cropping"

-   Top variables included Haddock SSB (1), Total Zooplankton Abundance, OISST (1), HW, and Mackerel

-   deviance explained was between 32-52 %

-   But several runs had "weird patterns" (signoidal relationship with zooplankton abundance and temperature), exponential with haddock and heat wave, which does make sense

**find image that goes here....! see slides but not sure where the file is**

-   Secondary runs split up zooplankton into cnidaria, krill, and calanus stage 5 in the GoM with a one year lag, now the marginal effects make more sense b/c they aren't

![](images/Partial%20Dependence.tiff)

### Preliminary modelling "conclusions"

-   Dropping the 0 & very low influence variables increased explained deviance

-   Reducing the list slowly/gradually doesn't really help much in terms of deviance explained when you start with too big a list

## Setup

```{r packages, echo = F} library(dplyr) library(ggplot2) library(corrplot) library(reshape2) library(ggstatsplot) library(gbm) library(tidyr) library(dismo) library(gridExtra) library(ggcorrplot)}
```

```{r data} # Load in the combined data  dat <- read.csv(here::here('Data/Combined.Data.csv')) # old version  # Load in the variable data and add lags # object is called newdat in the data prep script, but haven't exported a version in a minute}
```

## Preliminary plots

```{r ASAP.TS, echo=F, include=T} dat%>%    dplyr::select(year,recruits,R.no.devs,logR.dev, SR.std.resid, SSB, SPR, Rs)%>%   reshape2::melt(id.vars="year", variable.name="series")%>%   ggplot(aes(year,value)) +   geom_line()+    theme_bw()+   facet_wrap(~ series, scales = "free")}
```

```{r WHAM.TS, echo=F, include=T} ggplot(WHAM, aes(x=year, y=recrt))+   theme_bw()+     geom_line(na.rm=T, linewidth=1) +   geom_point(na.rm=T, size=1.5)+   scale_x_continuous(breaks = seq(0,2022,5),minor_breaks = seq(0,2022,1))+   xlab("Year") +   ylab("Recruitment")}
```

## Reruns with WHAM model (mm192) and VAST zooplankton indices

-   getting a few weird results....models where NAO index 5 year lag has 100 relative influence, models where dev explained is really low (like 0.6%) (but maybe I'm calculating it wrong)

-   "better" models when I use logRdev as opposed to raw recruitment values

-   Underwent a series of tuning processes where I changed values like the number of trees, minobsinnode, cv folds, etc etc --\> wish Id recorded better notes on how that process went but things changed when I used depth = 2, which I think is appropriate

-   less than 10 cv folds is better

-   

## Stock Recruit Relationship

```{r CCF, echo=FALSE, include=T}

ccf(dat$SSB_1, dat$recruits, plot = T, na.action = na.pass)
```

It appears that recruitment leads SSB by 3/4 years

## Tuning Strategy

1.  choose high learning rate (start at 0.1)

2.  determine optimum number of trees for this learning/shrinkage rate

3.  Fix tree hyperparameters, tune learning rate and assess speed vs. performance

-   Tree hyperparameters are minobsinnode and interaction depth (since I'm keeping depth to 1, only mess with minobs)

-   Actually, later runs with interaction depth = 2 had much higher deviance explained

4.  Tune tree-specific parameters (what are those...?) for decided learning rate

-   For learning rate of .001 at 1000 trees, optimum is 50

-   Reran with 50 as the number of trees and got what is actually a minimum value!

5.  lower learning rate again to see if there is any improvement in accuracy

## Learning to use new gbmt function

Nvm I'm sticking with just using gbm.fit/gbm...although why not use gbm.step with the new data....since I'm used to its syntax
